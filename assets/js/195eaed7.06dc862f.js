"use strict";(self.webpackChunkprojects_ieeebruins_com=self.webpackChunkprojects_ieeebruins_com||[]).push([[350],{1881:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var o=i(5893),t=i(1151);const r={},s="OpenCV",l={id:"pocket-racers/opencv",title:"OpenCV",description:"All instances of referring to a slide number, unless otherwise specified, are referring to the slides from Lecture 2 on Localization.",source:"@site/docs/pocket-racers/opencv.md",sourceDirName:"pocket-racers",slug:"/pocket-racers/opencv",permalink:"/projects.ieeebruins.com/pocket-racers/opencv",draft:!1,unlisted:!1,editUrl:"https://github.com/UCLA-IEEE/projects.ieeebruins.com/tree/main/docs/pocket-racers/opencv.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Module 4",permalink:"/projects.ieeebruins.com/pocket-racers/module-4"},next:{title:"DAV",permalink:"/projects.ieeebruins.com/DAV/"}},a={},c=[{value:"Setting up the camera on the Pi",id:"setting-up-the-camera-on-the-pi",level:2},{value:"Downscaling",id:"downscaling",level:2},{value:"Color conversion",id:"color-conversion",level:2},{value:"Box Blur",id:"box-blur",level:2},{value:"Gaussian Blur",id:"gaussian-blur",level:2},{value:"Simple thresholding",id:"simple-thresholding",level:2},{value:"Adaptive thresholding",id:"adaptive-thresholding",level:2},{value:"Thresholding with HSV",id:"thresholding-with-hsv",level:2},{value:"Bitwise mask operations",id:"bitwise-mask-operations",level:2},{value:"Erosion and Dilation",id:"erosion-and-dilation",level:2},{value:"Floodfill",id:"floodfill",level:2},{value:"Simple Blob Detection",id:"simple-blob-detection",level:2},{value:"Finding contours",id:"finding-contours",level:2},{value:"Contour area",id:"contour-area",level:2},{value:"Contour convex hull",id:"contour-convex-hull",level:2},{value:"Contour bounding rectangle",id:"contour-bounding-rectangle",level:2},{value:"Contour moment of inertia",id:"contour-moment-of-inertia",level:2}];function h(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"opencv",children:"OpenCV"}),"\n",(0,o.jsx)(n.p,{children:"All instances of referring to a slide number, unless otherwise specified, are referring to the slides from Lecture 2 on Localization."}),"\n",(0,o.jsx)(n.h2,{id:"setting-up-the-camera-on-the-pi",children:"Setting up the camera on the Pi"}),"\n",(0,o.jsx)(n.p,{children:"All of our Raspberry Pis come with Python 3 preinstalled. You just need to run these commands:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"OpenCV"}),". Install it using Pip by running the following command:"]}),"\n",(0,o.jsx)(n.p,{children:"pip3 install opencv-python"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Numpy"}),". You\u2019ll most likely_ _need to upgrade your version; this has been an issue on every Pi I tried working with. Run the following command:"]}),"\n",(0,o.jsx)(n.p,{children:"pip install -U numpy"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Some random other stuff"}),". The first time you do anything with OpenCV on the Pi, you\u2019ll get an error message about a missing file that you\u2019ve probably never heard of before. It\u2019s a simple fix, just run the following command (all on one line):"]}),"\n",(0,o.jsx)(n.p,{children:"sudo apt-get install libhdf5-dev libhdf5-serial-dev libatlas-base-dev libjasper-dev"}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["If, after running this command (and entering your password when prompted) OpenCV still doesn\u2019t work, visit this ",(0,o.jsx)(n.a,{href:"https://stackoverflow.com/questions/53347759/importerror-libcblas-so-3-cannot-open-shared-object-file-no-such-file-or-dire",children:"StackOverflow thread"}),". They mention a bunch of additional libraries you can try to install and see if that fixes things. The thread is a bit old, so you\u2019ll find that a lot of the libraries they mention don\u2019t exist anymore, but some of them should work."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Enable the Camera"}),". Finally, we need to enable the camera on our Pi. Run:"]}),"\n",(0,o.jsx)(n.p,{children:"sudo raspi-config"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"\t"}),"And navigate to Interface Options -> Legacy camera using the arrow keys. Press enter and reboot the Pi."]}),"\n",(0,o.jsx)(n.h2,{id:"downscaling",children:"Downscaling"}),"\n",(0,o.jsx)(n.p,{children:"new_dims = (int(img.shape[1] * scaleX), int(img.shape[0] * scaleY))\ndownscale = cv2.resize(img, new_dims)"}),"\n",(0,o.jsxs)(n.p,{children:["Annoyingly, the image ",(0,o.jsx)(n.code,{children:"shape"})," property (which contains the dimensions of the image array) is encoded as ",(0,o.jsx)(n.code,{children:"(height, width)"}),", but the resize function makes you supply a tuple in the form of ",(0,o.jsx)(n.code,{children:"(width, height)"}),". Make sure you get the indices right when multiplying by your scale factors, which should be between 0 and 1."]}),"\n",(0,o.jsx)(n.h2,{id:"color-conversion",children:"Color conversion"}),"\n",(0,o.jsx)(n.p,{children:"new_img = cv2.cvtColor(img, color_code)"}),"\n",(0,o.jsx)(n.p,{children:"Example Color Codes:"}),"\n",(0,o.jsx)(n.p,{children:"color_code = cv2.COLOR_BGR2GRAY\ncolor_code =  cv2.COLOR_BGR2HSV\ncolor_code = cv2.COLOR_BGR2HSV_FULL"}),"\n",(0,o.jsxs)(n.p,{children:["For the full list of color codes, see ",(0,o.jsx)(n.a,{href:"https://docs.opencv.org/3.4/d8/d01/group__imgproc__color__conversions.html",children:"this page"}),". There are a ",(0,o.jsx)(n.em,{children:"lot"})," of color conversion formats here, but for grayscaling you\u2019ll use ",(0,o.jsx)(n.code,{children:"cv2.COLOR_BGR2GRAY"})," and for HSV you\u2019ll use ",(0,o.jsx)(n.code,{children:"cv2.COLOR_BGR2HSV"})," or ",(0,o.jsx)(n.code,{children:"cv2.COLOR_BGR2HSV_FULL"}),". Note that OpenCV encodes your images by default as BGR (blue-green-red) and not RGB, so you have to use the appropriate conversions."]}),"\n",(0,o.jsx)(n.h2,{id:"box-blur",children:"Box Blur"}),"\n",(0,o.jsx)("code",{children:(0,o.jsx)("em",{children:"blur = cv2.blur(img,(n,m))"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"(n,m)"})," is the size of your blurring kernel. For example, ",(0,o.jsx)(n.code,{children:"(3, 3)"})," would indicate that each pixel is getting replaced with the average of the 3x3 box around it \u2013 a small amount of blurring. Something like ",(0,o.jsx)(n.code,{children:"(13, 13)"}),", on the other hand, would blur each pixel with a 13x13 box of pixels around it, which is much more significant. However, they don\u2019t have to be equal \u2013 if you really wanted to, you could do a rectangular blurring kernel. Try things and see what they do!"]}),"\n",(0,o.jsx)(n.h2,{id:"gaussian-blur",children:"Gaussian Blur"}),"\n",(0,o.jsx)("code",{children:"blur = cv2.GaussianBlur(img,ksize=(k,k),sigmaX=0)"}),"\n",(0,o.jsxs)(n.p,{children:["Here, ksize is the side length of our kernel (must be odd). Sigma X is the standard deviation in the X direction and if set to 0 is automatically calculated from ksize (sigmaY defaults to the same as sigmaX) For more details, visit ",(0,o.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1",children:"this page"}),". For the functional difference between box blurring and Gaussian blurring, see slide 23."]}),"\n",(0,o.jsxs)(n.p,{children:["For additional blurring techniques, visit ",(0,o.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html",children:"this page"}),"."]}),"\n",(0,o.jsx)(n.h2,{id:"simple-thresholding",children:"Simple thresholding"}),"\n",(0,o.jsx)(n.p,{children:"_, new_img = cv2.threshold(src, thresh, maxval, type)\ntype = cv2.THRESH_BINARY_INV or cv2.THRESH_BINARY"}),"\n",(0,o.jsxs)(n.p,{children:["This is done on a grayscale image. When the thresholding type is set to ",(0,o.jsx)(n.code,{children:"cv2.THRESH_BINARY"}),", it sets all pixels above ",(0,o.jsx)(n.code,{children:"thresh"})," to ",(0,o.jsx)(n.code,{children:"maxval"})," and all pixels below it to 0. When it\u2019s set to ",(0,o.jsx)(n.code,{children:"cv2.THRESH_BINARY_INV"}),", the function sets all pixels above ",(0,o.jsx)(n.code,{children:"thresh"})," to 0 and all pixels below it to 255."]}),"\n",(0,o.jsx)(n.h2,{id:"adaptive-thresholding",children:"Adaptive thresholding"}),"\n",(0,o.jsx)(n.p,{children:"mask = cv2.adaptiveThreshold(im, maxValue=255,\nadaptiveMethod=cv2.ADAPTIVE_THRESH_GAUSSIAN_C, thresholdType=cv2.THRESH_BINARY_INV, blockSize=k, C=t)"}),"\n",(0,o.jsxs)(n.p,{children:["This function implements adaptive thresholding; the values you will be playing with should be ",(0,o.jsx)(n.code,{children:"blockSize"})," and ",(0,o.jsx)(n.code,{children:"C"})," (maybe also the adaptive method). C is a constant that is ",(0,o.jsx)(n.strong,{children:"subtracted"})," from the adaptive threshold."]}),"\n",(0,o.jsxs)(n.p,{children:["For all the different types of thresholding in OpenCV, visit ",(0,o.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html",children:"this page"}),"."]}),"\n",(0,o.jsxs)(n.admonition,{type:"note",children:[(0,o.jsx)(n.p,{children:"There\u2019s been some confusion about how the C value works exactly. Basically, the adaptive thresholding function operates on every pixel in the original image as follows:"}),(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Calculate a threshold by taking the Gaussian mean of the values in a ",(0,o.jsx)(n.code,{children:"k"}),"-by-",(0,o.jsx)(n.code,{children:"k"})," block centered on the pixel in question."]}),"\n",(0,o.jsx)(n.li,{children:"Subtract this value by C."}),"\n",(0,o.jsxs)(n.li,{children:["Compare the resulting threshold value with the value of the pixel and act in accordance with ",(0,o.jsx)(n.code,{children:"thresholdType"}),"."]}),"\n"]})]}),"\n",(0,o.jsx)(n.h2,{id:"thresholding-with-hsv",children:"Thresholding with HSV"}),"\n",(0,o.jsx)(n.admonition,{type:"info",children:(0,o.jsxs)(n.p,{children:["The range of hues in OpenCV is ",(0,o.jsx)(n.strong,{children:"0 to 180"}),", not 0 to 255. Using values above 180, to the best of our knowledge, don\u2019t really do anything."]})}),"\n",(0,o.jsx)("code",{children:(0,o.jsx)("em",{children:"mask = cv2.inRange(hsv, lowerBound, upperBound)"})}),"\n",(0,o.jsxs)(n.p,{children:["The traditional threshold functions don\u2019t really work for HSV images, so we use the ",(0,o.jsx)(n.code,{children:"cv2.inRange()"})," function instead. Here, lowerBound and upperBound are three-tuples. A range of, for example,"]}),"\n",(0,o.jsx)(n.p,{children:"cv2.inRange(hsv, (135, 0, 0), (180, 255, 255))"}),"\n",(0,o.jsxs)(n.p,{children:["would create a mask preserving only those pixels that fall between the hue range of 135-180; in other words, purple to red. Since the saturation and value ranges are 0-255, pixels of any saturation and value will be preserved in the mask. When you display a mask using ",(0,o.jsx)(n.code,{children:"cv2.imshow()"}),", it\u2019ll give you an image that\u2019s white at all the pixels that meet the range criteria and black everywhere else. Refer to slide 38 in the lecture for an example."]}),"\n",(0,o.jsx)(n.h2,{id:"bitwise-mask-operations",children:"Bitwise mask operations"}),"\n",(0,o.jsx)("code",{children:(0,o.jsxs)("em",{children:["mask = np.bitwise_and(x1, x2) ",(0,o.jsx)(n.br,{}),"\n","mask = np.bitwise_or(x1, x2)"]})}),"\n",(0,o.jsx)(n.p,{children:"mask = np.bitwise_not(x1, x2)\nresult = cv2.bitwise_and(x1, x1, mask=x2)"}),"\n",(0,o.jsx)(n.p,{children:"These work about as you\u2019d expect them to, except instead of operating on two bits they operate on two arrays of bits. See slide 39 for a visual explanation of their functions."}),"\n",(0,o.jsx)(n.p,{children:"You\u2019ll note that the syntax for the operation combining a mask with an image is slightly different from the operations on just masks themselves. This is because in Python and OpenCV, your image will typically be represented as an array of 3 \u201cchannels\u201d, for R-G-B or H-S-V. However, you can\u2019t combine a 3-channel image with a 1-channel mask (because it\u2019s just black and white) using Numpy, so you have to use the OpenCV function instead."}),"\n",(0,o.jsx)(n.h2,{id:"erosion-and-dilation",children:"Erosion and Dilation"}),"\n",(0,o.jsx)(n.p,{children:"res = cv2.erode(img, np.ones((k,k), np.uint8), iterations=n)\nres = cv2.dilate(img, np.ones((k,k), np.uint8), iterations=n)"}),"\n",(0,o.jsxs)(n.p,{children:["Like with previous functions, ",(0,o.jsx)(n.code,{children:"(k,k)"})," is the block size. ",(0,o.jsx)(n.code,{children:"n"})," is a whole number representing the number of times you want your erosion or dilation to be applied to the image."]}),"\n",(0,o.jsx)(n.h2,{id:"floodfill",children:"Floodfill"}),"\n",(0,o.jsx)(n.p,{children:"h, w = img.shape[:2]\nmask = np.zeros((h + 2, w + 2), np.uint8)\ncv2.floodFill(img, mask, (j, k), color)"}),"\n",(0,o.jsx)(n.p,{children:"Floodfill is an algorithm that essentially starts at a certain point and \u201cspreads out\u201d to all the adjacent points in the image that are equal to it. You can think of it as doing the exact same thing that the Fill Bucket tool does in computer drawing programs."}),"\n",(0,o.jsxs)(n.p,{children:["Here, the floodfill function modifies the ",(0,o.jsx)(n.code,{children:"img"})," parameter directly \u2013 in other words, it ",(0,o.jsx)(n.strong,{children:"does not return a new image."})," The ",(0,o.jsx)(n.code,{children:"mask"})," parameter will in almost all cases be a slightly bigger but empty image; in practice, nonzero values in the mask specify \u201cwalls\u201d that the floodfill algorithm can\u2019t cross. Finally, ",(0,o.jsx)(n.code,{children:"(j,k)"})," is the starting point (the function will fill whatever ",(0,o.jsx)(n.em,{children:"closed"})," region contains that point) and ",(0,o.jsx)(n.code,{children:"color"})," is the color value (from 0-255) that you want to fill the region with."]}),"\n",(0,o.jsxs)(n.p,{children:["Note that in the starting point ",(0,o.jsx)(n.code,{children:"(j,k)"}),", ",(0,o.jsx)(n.code,{children:"j"})," represents the column and ",(0,o.jsx)(n.code,{children:"k"})," represents the row \u2013 like an XY-coordinate pair starting at the top left of the image and growing downwards."]}),"\n",(0,o.jsx)(n.h2,{id:"simple-blob-detection",children:"Simple Blob Detection"}),"\n",(0,o.jsx)(n.p,{children:"bdp = cv2.SimpleBlobDetector_Params()\nbdp.filterByArea = False\nbdp.filterByConvexity = False\nbdp.filterByCircularity = True\nbdp.filterByInertia = False\nbdp.filterByColor = True\nbdp.blobColor = 255\nbdp.minCircularity = 0.5\nbdp.maxCircularity = 1\ndetector = cv2.SimpleBlobDetector_create(bdp)\npoints = detector.detect(mask)"}),"\n",(0,o.jsxs)(n.p,{children:["The above code extracts all white blobs with a circularity greater than 0.5. You can read more about the different features ",(0,o.jsx)(n.a,{href:"https://docs.opencv.org/3.4/d0/d7a/classcv_1_1SimpleBlobDetector.html",children:"here"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["The list of all available parameters for the blob detector can be found ",(0,o.jsx)(n.a,{href:"https://docs.opencv.org/3.4/d8/da7/structcv_1_1SimpleBlobDetector_1_1Params.html",children:"here"})," (and the ones we use are listed below). To add one of the above filters, set it to ",(0,o.jsx)(n.code,{children:"True"})," instead of ",(0,o.jsx)(n.code,{children:"False"})," as shown above, and then update the relevant minimum and maximum parameters. The documentation is for C++, but the names should be the same (and they\u2019re listed below); and if a certain parameter doesn\u2019t work, a little Googling goes a long way :-)"]}),"\n",(0,o.jsxs)(n.p,{children:["So far, we\u2019ve used the following ",(0,o.jsxs)("code",{children:["SimpleBlobDetector_Params",(0,o.jsx)("em",{})]})," members:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"bool \tfilterByArea"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tminArea"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tmaxArea "})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"bool \tfilterByCircularity"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tminCircularity"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tmaxCircularity"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"bool \tfilterByInertia"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tminInertiaRatio"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tmaxInertiaRatio"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"bool \tfilterByColor"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"int\t\tblobColor"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"bool \tfilterByConvexity"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tminConvexity"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.code,{children:"float \tmaxConvexity"})}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"finding-contours",children:"Finding contours"}),"\n",(0,o.jsx)(n.p,{children:"contours, hierarchy = cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"contours"})," is a list of all the contours in the image."]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"hierarchy"})," is a 2D array containing hierarchical information about each contour. In the following diagram, ",(0,o.jsx)("code",{children:(0,o.jsx)("em",{children:"i"})})," is the index of a given contour in the list ",(0,o.jsx)("code",{children:"contours"}),". ",(0,o.jsx)("code",{children:"hierarchy"})," is actually an array containing the tree of contours, so the data we\u2019re actually concerned with is in ",(0,o.jsx)("code",{children:"hierarchy[0]"}),". The ",(0,o.jsx)("code",{children:"i"}),"-th element in the tree is an array of 4 elements: the next sibling, previous sibling, first child, and parent, respectively. If any of these elements don\u2019t exist (for example, the parent of the root contour or any node with no siblings), it\u2019ll be ",(0,o.jsx)("code",{children:"-1"})," in the array."]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"drawing",src:i(648).Z+"",width:"1088",height:"908"})}),"\n",(0,o.jsx)(n.h2,{id:"contour-area",children:"Contour area"}),"\n",(0,o.jsx)(n.p,{children:"area = cv2.contourArea(contours[i])"}),"\n",(0,o.jsxs)(n.p,{children:["This function returns the area of the contour, in pixels. ",(0,o.jsx)(n.code,{children:"contours[i]"}),", of course, is the ",(0,o.jsx)(n.em,{children:"i"}),"-th contour in the list of contours."]}),"\n",(0,o.jsx)(n.h2,{id:"contour-convex-hull",children:"Contour convex hull"}),"\n",(0,o.jsx)(n.p,{children:"hull = cv2.convexHull(contours[i])"}),"\n",(0,o.jsx)(n.p,{children:"Remember that the convex hull of a blob is a purely convex polygon that encloses a contour. To find the convexity of a contour, you calculate the ratio of the area of the contour to the area of its convex hull."}),"\n",(0,o.jsx)(n.h2,{id:"contour-bounding-rectangle",children:"Contour bounding rectangle"}),"\n",(0,o.jsx)(n.p,{children:"x,y,w,h = cv2.boundingRect(contours[i])"}),"\n",(0,o.jsx)(n.p,{children:"The regularly-oriented bounding rectangle of a contour isn\u2019t as \u201ctightly wrapped\u201d as the convex hull, so to speak. Typically the formula for calculating a bounding rectangle involves finding the leftmost and rightmost x-coordinates in the blob, finding the topmost and bottommost y-coordinates, and creating a rectangle using those coordinates to determine:"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"x"})," \u2013 the ",(0,o.jsx)(n.em,{children:"x"}),"-coordinate of the top-left corner of the rectangle"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"y"})," \u2013 the ",(0,o.jsx)(n.em,{children:"y"}),"-coordinate of the top-left corner of the rectangle"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"w"})," \u2013 the width of the rectangle"]}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.code,{children:"h"})," \u2013 the height of the rectangle"]}),"\n",(0,o.jsx)(n.p,{children:"This will typically result in the bounding rectangle having more excess area than the convex hull of a given contour."}),"\n",(0,o.jsx)(n.h2,{id:"contour-moment-of-inertia",children:"Contour moment of inertia"}),"\n",(0,o.jsx)(n.p,{children:'moments = cv2.moments(contours[i])\nmx = int(moments["m10"] / moments["m00"])\nmy = int(moments["m01"] / moments["m00"])'}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.code,{children:"cv2.moments"})," function actually returns some more complicated data than what we\u2019ll have to use, but these are the formulas for calculating our needed coordinates. ",(0,o.jsx)(n.code,{children:"mx"})," and ",(0,o.jsx)(n.code,{children:"my"})," are the x and y-coordinates of the moment of inertia of the contour at ",(0,o.jsx)(n.code,{children:"contours[i]"}),"."]}),"\n",(0,o.jsx)(n.admonition,{type:"note",children:(0,o.jsxs)(n.p,{children:["Sometimes, you\u2019ll get a DivideByZero error from either of these lines in your code. The issue here is that, if your contours don\u2019t separate properly, the value ",(0,o.jsx)(n.code,{children:'moments["m00"]'})," will be zero and your division will cause an error. To fix this, you can simply wrap these calculations in a ",(0,o.jsx)(n.code,{children:"try/except"})," block, handling the error as you see fit."]})}),"\n",(0,o.jsxs)(n.p,{children:["The guide ",(0,o.jsx)(n.a,{href:"https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html",children:"here"})," has additional functions that you can use with contours. As we use more, we\u2019ll add them to these docs."]})]})}function d(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(h,{...e})}):h(e)}},648:(e,n,i)=>{i.d(n,{Z:()=>o});const o=i.p+"assets/images/image7-42e526ad0b1b2d3fc2a1cd7f1c829524.png"},1151:(e,n,i)=>{i.d(n,{Z:()=>l,a:()=>s});var o=i(7294);const t={},r=o.createContext(t);function s(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);